{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Implementation of GCN.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atwine/GCN/blob/master/Implementation_of_GCN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMKkM772wPcB",
        "colab_type": "text"
      },
      "source": [
        "# Graph Convolution Network\n",
        "\n",
        "These are used to help work with data which has not euclidian distances, such as data that deals with protein structures and also network relationships"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oODLvevPsUkk",
        "colab_type": "text"
      },
      "source": [
        "## Clone the necessary lib\n",
        "\n",
        "The supporting codes used in the notebook are from GitHub as seen below.\n",
        "\n",
        "A special thanks to the people that worked on them to help make this process easy for me. I am a learner so I just did a replication of the experiment so I can understand some of the processes involved."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmNfENvAsa4a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os  #to play around with folders"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZVgFJBrsc7N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "68e57a80-b6db-4b84-f40f-23cd2c2eb36b"
      },
      "source": [
        "#Install Kegra\n",
        "!git clone https://github.com/Awannaphasch2016/kegra_gcn.git"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'kegra_gcn'...\n",
            "remote: Enumerating objects: 30, done.\u001b[K\n",
            "remote: Counting objects:   3% (1/30)\u001b[K\rremote: Counting objects:   6% (2/30)\u001b[K\rremote: Counting objects:  10% (3/30)\u001b[K\rremote: Counting objects:  13% (4/30)\u001b[K\rremote: Counting objects:  16% (5/30)\u001b[K\rremote: Counting objects:  20% (6/30)\u001b[K\rremote: Counting objects:  23% (7/30)\u001b[K\rremote: Counting objects:  26% (8/30)\u001b[K\rremote: Counting objects:  30% (9/30)\u001b[K\rremote: Counting objects:  33% (10/30)\u001b[K\rremote: Counting objects:  36% (11/30)\u001b[K\rremote: Counting objects:  40% (12/30)\u001b[K\rremote: Counting objects:  43% (13/30)\u001b[K\rremote: Counting objects:  46% (14/30)\u001b[K\rremote: Counting objects:  50% (15/30)\u001b[K\rremote: Counting objects:  53% (16/30)\u001b[K\rremote: Counting objects:  56% (17/30)\u001b[K\rremote: Counting objects:  60% (18/30)\u001b[K\rremote: Counting objects:  63% (19/30)\u001b[K\rremote: Counting objects:  66% (20/30)\u001b[K\rremote: Counting objects:  70% (21/30)\u001b[K\rremote: Counting objects:  73% (22/30)\u001b[K\rremote: Counting objects:  76% (23/30)\u001b[K\rremote: Counting objects:  80% (24/30)\u001b[K\rremote: Counting objects:  83% (25/30)\u001b[K\rremote: Counting objects:  86% (26/30)\u001b[K\rremote: Counting objects:  90% (27/30)\u001b[K\rremote: Counting objects:  93% (28/30)\u001b[K\rremote: Counting objects:  96% (29/30)\u001b[K\rremote: Counting objects: 100% (30/30)\u001b[K\rremote: Counting objects: 100% (30/30), done.\u001b[K\n",
            "remote: Compressing objects: 100% (27/27), done.\u001b[K\n",
            "remote: Total 30 (delta 1), reused 30 (delta 1), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (30/30), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUPi9A_IshAg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir('kegra_gcn')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Kc-7aQntJxb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dfaa723a-60ab-4144-b6f7-64141e4b78b0"
      },
      "source": [
        "!python setup.py install"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "creating kegra.egg-info\n",
            "writing kegra.egg-info/PKG-INFO\n",
            "writing dependency_links to kegra.egg-info/dependency_links.txt\n",
            "writing requirements to kegra.egg-info/requires.txt\n",
            "writing top-level names to kegra.egg-info/top_level.txt\n",
            "writing manifest file 'kegra.egg-info/SOURCES.txt'\n",
            "writing manifest file 'kegra.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_py\n",
            "creating build\n",
            "creating build/lib\n",
            "creating build/lib/kegra\n",
            "copying kegra/anak.py -> build/lib/kegra\n",
            "copying kegra/__init__.py -> build/lib/kegra\n",
            "copying kegra/utils.py -> build/lib/kegra\n",
            "copying kegra/draw_graph.py -> build/lib/kegra\n",
            "copying kegra/train.py -> build/lib/kegra\n",
            "copying kegra/random_code.py -> build/lib/kegra\n",
            "creating build/lib/kegra/layers\n",
            "copying kegra/layers/graph.py -> build/lib/kegra/layers\n",
            "copying kegra/layers/__init__.py -> build/lib/kegra/layers\n",
            "creating build/lib/kegra/testing\n",
            "copying kegra/testing/__init__.py -> build/lib/kegra/testing\n",
            "copying kegra/testing/sparse_matrix.py -> build/lib/kegra/testing\n",
            "creating build/bdist.linux-x86_64\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/kegra\n",
            "copying build/lib/kegra/anak.py -> build/bdist.linux-x86_64/egg/kegra\n",
            "creating build/bdist.linux-x86_64/egg/kegra/layers\n",
            "copying build/lib/kegra/layers/graph.py -> build/bdist.linux-x86_64/egg/kegra/layers\n",
            "copying build/lib/kegra/layers/__init__.py -> build/bdist.linux-x86_64/egg/kegra/layers\n",
            "copying build/lib/kegra/__init__.py -> build/bdist.linux-x86_64/egg/kegra\n",
            "creating build/bdist.linux-x86_64/egg/kegra/testing\n",
            "copying build/lib/kegra/testing/__init__.py -> build/bdist.linux-x86_64/egg/kegra/testing\n",
            "copying build/lib/kegra/testing/sparse_matrix.py -> build/bdist.linux-x86_64/egg/kegra/testing\n",
            "copying build/lib/kegra/utils.py -> build/bdist.linux-x86_64/egg/kegra\n",
            "copying build/lib/kegra/draw_graph.py -> build/bdist.linux-x86_64/egg/kegra\n",
            "copying build/lib/kegra/train.py -> build/bdist.linux-x86_64/egg/kegra\n",
            "copying build/lib/kegra/random_code.py -> build/bdist.linux-x86_64/egg/kegra\n",
            "byte-compiling build/bdist.linux-x86_64/egg/kegra/anak.py to anak.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/kegra/layers/graph.py to graph.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/kegra/layers/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/kegra/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/kegra/testing/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/kegra/testing/sparse_matrix.py to sparse_matrix.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/kegra/utils.py to utils.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/kegra/draw_graph.py to draw_graph.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/kegra/train.py to train.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/kegra/random_code.py to random_code.cpython-36.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying kegra.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying kegra.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying kegra.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying kegra.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying kegra.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "creating dist\n",
            "creating 'dist/kegra-0.0.1-py3.6.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing kegra-0.0.1-py3.6.egg\n",
            "Copying kegra-0.0.1-py3.6.egg to /usr/local/lib/python3.6/dist-packages\n",
            "Adding kegra 0.0.1 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/kegra-0.0.1-py3.6.egg\n",
            "Processing dependencies for kegra==0.0.1\n",
            "Searching for Keras==2.2.5\n",
            "Best match: Keras 2.2.5\n",
            "Adding Keras 2.2.5 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for PyYAML==3.13\n",
            "Best match: PyYAML 3.13\n",
            "Adding PyYAML 3.13 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for scipy==1.3.1\n",
            "Best match: scipy 1.3.1\n",
            "Adding scipy 1.3.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for numpy==1.16.5\n",
            "Best match: numpy 1.16.5\n",
            "Adding numpy 1.16.5 to easy-install.pth file\n",
            "Installing f2py script to /usr/local/bin\n",
            "Installing f2py3 script to /usr/local/bin\n",
            "Installing f2py3.6 script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for h5py==2.8.0\n",
            "Best match: h5py 2.8.0\n",
            "Adding h5py 2.8.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for Keras-Preprocessing==1.1.0\n",
            "Best match: Keras-Preprocessing 1.1.0\n",
            "Adding Keras-Preprocessing 1.1.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for six==1.12.0\n",
            "Best match: six 1.12.0\n",
            "Adding six 1.12.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for Keras-Applications==1.0.8\n",
            "Best match: Keras-Applications 1.0.8\n",
            "Adding Keras-Applications 1.0.8 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Finished processing dependencies for kegra==0.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIqy5XmktNyI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir('/content/kegra_gcn/GCN-tutorial')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COeamlCMtRUg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "50297567-b78d-448a-ec20-ca333956c263"
      },
      "source": [
        "#this must be in the kegra folder otherwise there will be errors loading the dataset needed.\n",
        "!git clone https://github.com/C-opt/GCN-tutorial.git"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'GCN-tutorial'...\n",
            "remote: Enumerating objects: 30, done.\u001b[K\n",
            "remote: Counting objects:   3% (1/30)\u001b[K\rremote: Counting objects:   6% (2/30)\u001b[K\rremote: Counting objects:  10% (3/30)\u001b[K\rremote: Counting objects:  13% (4/30)\u001b[K\rremote: Counting objects:  16% (5/30)\u001b[K\rremote: Counting objects:  20% (6/30)\u001b[K\rremote: Counting objects:  23% (7/30)\u001b[K\rremote: Counting objects:  26% (8/30)\u001b[K\rremote: Counting objects:  30% (9/30)\u001b[K\rremote: Counting objects:  33% (10/30)\u001b[K\rremote: Counting objects:  36% (11/30)\u001b[K\rremote: Counting objects:  40% (12/30)\u001b[K\rremote: Counting objects:  43% (13/30)\u001b[K\rremote: Counting objects:  46% (14/30)\u001b[K\rremote: Counting objects:  50% (15/30)\u001b[K\rremote: Counting objects:  53% (16/30)\u001b[K\rremote: Counting objects:  56% (17/30)\u001b[K\rremote: Counting objects:  60% (18/30)\u001b[K\rremote: Counting objects:  63% (19/30)\u001b[K\rremote: Counting objects:  66% (20/30)\u001b[K\rremote: Counting objects:  70% (21/30)\u001b[K\rremote: Counting objects:  73% (22/30)\u001b[K\rremote: Counting objects:  76% (23/30)\u001b[K\rremote: Counting objects:  80% (24/30)\u001b[K\rremote: Counting objects:  83% (25/30)\u001b[K\rremote: Counting objects:  86% (26/30)\u001b[K\rremote: Counting objects:  90% (27/30)\u001b[K\rremote: Counting objects:  93% (28/30)\u001b[K\rremote: Counting objects:  96% (29/30)\u001b[K\rremote: Counting objects: 100% (30/30)\u001b[K\rremote: Counting objects: 100% (30/30), done.\u001b[K\n",
            "remote: Compressing objects:   3% (1/26)\u001b[K\rremote: Compressing objects:   7% (2/26)\u001b[K\rremote: Compressing objects:  11% (3/26)\u001b[K\rremote: Compressing objects:  15% (4/26)\u001b[K\rremote: Compressing objects:  19% (5/26)\u001b[K\rremote: Compressing objects:  23% (6/26)\u001b[K\rremote: Compressing objects:  26% (7/26)\u001b[K\rremote: Compressing objects:  30% (8/26)\u001b[K\rremote: Compressing objects:  34% (9/26)\u001b[K\rremote: Compressing objects:  38% (10/26)\u001b[K\rremote: Compressing objects:  42% (11/26)\u001b[K\rremote: Compressing objects:  46% (12/26)\u001b[K\rremote: Compressing objects:  50% (13/26)\u001b[K\rremote: Compressing objects:  53% (14/26)\u001b[K\rremote: Compressing objects:  57% (15/26)\u001b[K\rremote: Compressing objects:  61% (16/26)\u001b[K\rremote: Compressing objects:  65% (17/26)\u001b[K\rremote: Compressing objects:  69% (18/26)\u001b[K\rremote: Compressing objects:  73% (19/26)\u001b[K\rremote: Compressing objects:  76% (20/26)\u001b[K\rremote: Compressing objects:  80% (21/26)\u001b[K\rremote: Compressing objects:  84% (22/26)\u001b[K\rremote: Compressing objects:  88% (23/26)\u001b[K\rremote: Compressing objects:  92% (24/26)\u001b[K\rremote: Compressing objects:  96% (25/26)\u001b[K\rremote: Compressing objects: 100% (26/26)\u001b[K\rremote: Compressing objects: 100% (26/26), done.\u001b[K\n",
            "Unpacking objects:   3% (1/30)   \rUnpacking objects:   6% (2/30)   \rUnpacking objects:  10% (3/30)   \rUnpacking objects:  13% (4/30)   \rUnpacking objects:  16% (5/30)   \rUnpacking objects:  20% (6/30)   \rUnpacking objects:  23% (7/30)   \rUnpacking objects:  26% (8/30)   \rUnpacking objects:  30% (9/30)   \rUnpacking objects:  33% (10/30)   \rUnpacking objects:  36% (11/30)   \rUnpacking objects:  40% (12/30)   \rUnpacking objects:  43% (13/30)   \rUnpacking objects:  46% (14/30)   \rUnpacking objects:  50% (15/30)   \rremote: Total 30 (delta 7), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects:  53% (16/30)   \rUnpacking objects:  56% (17/30)   \rUnpacking objects:  60% (18/30)   \rUnpacking objects:  63% (19/30)   \rUnpacking objects:  66% (20/30)   \rUnpacking objects:  70% (21/30)   \rUnpacking objects:  73% (22/30)   \rUnpacking objects:  76% (23/30)   \rUnpacking objects:  80% (24/30)   \rUnpacking objects:  83% (25/30)   \rUnpacking objects:  86% (26/30)   \rUnpacking objects:  90% (27/30)   \rUnpacking objects:  93% (28/30)   \rUnpacking objects:  96% (29/30)   \rUnpacking objects: 100% (30/30)   \rUnpacking objects: 100% (30/30), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LyB3fGztSk9",
        "colab_type": "text"
      },
      "source": [
        "# Notebook Implementation\n",
        "\n",
        "Here I implement the notebook I learnt from, I haven't altered the codes at all, I will do in due course as I better understand the concept of GCN\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGUs1t3UtUhP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "from keras.layers import Input, Dropout\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "from keras.regularizers import l2\n",
        "\n",
        "from kegra.layers.graph import GraphConvolution\n",
        "import utils\n",
        "\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEGmluL-tXK-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATASET = 'cora'\n",
        "FILTER = 'localpool'  # 'chebyshev'\n",
        "MAX_DEGREE = 2  # maximum polynomial degree\n",
        "SYM_NORM = True  # symmetric (True) vs. left-only (False) normalization\n",
        "NB_EPOCH = 500\n",
        "PATIENCE = 10  # early stopping patience"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2QLeu0RuSCI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "d2607d17-8fb3-44d3-b32d-aff51b74cd99"
      },
      "source": [
        "X, A, y = utils.load_data(dataset=DATASET)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading cora dataset...\n",
            "Dataset has 2708 nodes, 5429 edges, 1433 features.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckKN6WNXugWN",
        "colab_type": "text"
      },
      "source": [
        "Split dataset into train, validation, and test batches. The focus here is to split only the labels, not X or A itself. We can sample the training data in X and A by train_mask."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwGF-xobuUIF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train, y_val, y_test, idx_train, idx_val, idx_test, train_mask = utils.get_splits(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94SvSYOcujPQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155
        },
        "outputId": "cfb6549c-b868-4659-9dac-691151df296c"
      },
      "source": [
        "\n",
        "print(\"X dim={}\".format(X.shape))\n",
        "print(\"A dim={}\".format(A.shape))\n",
        "print(\"y dim={}\".format(y.shape))\n",
        "\n",
        "print(\"y_train dim={}\".format(y_train.shape))\n",
        "print(\"y_val dim={}\".format(y_val.shape))\n",
        "print(\"y_test dim={}\".format(y_test.shape))\n",
        "\n",
        "tmp_sum = 0\n",
        "for i in range(train_mask.shape[0]):\n",
        "    if train_mask[i] == True:\n",
        "        tmp_sum+=1\n",
        "\n",
        "print(tmp_sum)\n",
        "print(train_mask)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X dim=(2708, 1433)\n",
            "A dim=(2708, 2708)\n",
            "y dim=(2708, 7)\n",
            "y_train dim=(2708, 7)\n",
            "y_val dim=(2708, 7)\n",
            "y_test dim=(2708, 7)\n",
            "80\n",
            "[ True  True  True ... False False False]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0LEWvvTupyT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Normalize X\n",
        "from numpy import linalg as LA\n",
        "for i in range(X.shape[0]):\n",
        "    X[i] /= LA.norm(X[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egR8WBLiu4pR",
        "colab_type": "text"
      },
      "source": [
        "Take a peek at the 100-th one-hot embedded paper after performing normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HVK8QrquuZA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "outputId": "6955660f-515d-4450-dd81-58cd7eb7b03d"
      },
      "source": [
        "utils.look_sparse_matrix(X, 100)\n",
        "\n",
        "#from what i understand in the information below, all the other indexes are linked to the paper 0 in the graph which makes this really interesting"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "62:0.20000000298023224\n",
            "99:0.20000000298023224\n",
            "132:0.20000000298023224\n",
            "142:0.20000000298023224\n",
            "292:0.20000000298023224\n",
            "402:0.20000000298023224\n",
            "462:0.20000000298023224\n",
            "495:0.20000000298023224\n",
            "507:0.20000000298023224\n",
            "575:0.20000000298023224\n",
            "648:0.20000000298023224\n",
            "675:0.20000000298023224\n",
            "724:0.20000000298023224\n",
            "733:0.20000000298023224\n",
            "778:0.20000000298023224\n",
            "779:0.20000000298023224\n",
            "821:0.20000000298023224\n",
            "1071:0.20000000298023224\n",
            "1097:0.20000000298023224\n",
            "1151:0.20000000298023224\n",
            "1230:0.20000000298023224\n",
            "1331:0.20000000298023224\n",
            "1334:0.20000000298023224\n",
            "1348:0.20000000298023224\n",
            "1422:0.20000000298023224\n",
            "tmp_sum=[[1.]]==1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8M2silJu7Gn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1334fa60-7852-41a8-cf2d-8ba473023899"
      },
      "source": [
        "if FILTER == 'localpool':\n",
        "    \"\"\" Local pooling filters (see 'renormalization trick' in Kipf & Welling, arXiv 2016) \"\"\"\n",
        "    print('Using local pooling filters...')\n",
        "    A_hat = utils.preprocess_adj(A, SYM_NORM)\n",
        "    support = 1\n",
        "    graph = [X, A_hat]\n",
        "elif FILTER == 'chebyshev':\n",
        "    \"\"\" Chebyshev polynomial basis filters (Defferard et al., NIPS 2016)  \"\"\"\n",
        "    print('Using Chebyshev polynomial basis filters...')\n",
        "    L = normalized_laplacian(A, SYM_NORM)\n",
        "    L_scaled = rescale_laplacian(L)\n",
        "    T_k = chebyshev_polynomial(L_scaled, MAX_DEGREE)\n",
        "    support = MAX_DEGREE + 1\n",
        "    graph = [X]+T_k\n",
        "    \n",
        "else:\n",
        "    raise Exception('Invalid filter type.')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using local pooling filters...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lXWSPHXvr9t",
        "colab_type": "text"
      },
      "source": [
        "# Modeling done here\n",
        "\n",
        "This is where the magic happens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hux1YgyUvAVW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Define input tensor\n",
        "A_hat_in = Input(shape=A_hat.shape, batch_shape=(None, None), sparse=True)\n",
        "X_in = Input(shape=(X.shape[1],))\n",
        "\n",
        "# Define model architecture\n",
        "# NOTE: We pass arguments for graph convolutional layers as a list of tensors.\n",
        "# This is somewhat hacky, more elegant options would require rewriting the Layer base class.\n",
        "H_1 = Dropout(0.5)(X_in)\n",
        "H_2 = GraphConvolution(16, support, activation='relu', kernel_regularizer=l2(5e-4))([H_1]+[A_hat_in])\n",
        "H_2 = Dropout(0.5)(H_2)\n",
        "Y_out = GraphConvolution(y.shape[1], support, activation='softmax')([H_2]+[A_hat_in]) #H_{l+1} = f_act(H_l, A)\n",
        "\n",
        "# Compile model\n",
        "model = Model(inputs=[X_in]+[A_hat_in], outputs=Y_out)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.01))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-JdXXI7vHlD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "outputId": "767c9886-9aa8-4c9b-fb1b-08cbf14a5e73"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            (None, 1433)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 1433)         0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_3 (InputLayer)            (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "graph_convolution_3 (GraphConvo (None, 16)           22944       dropout_3[0][0]                  \n",
            "                                                                 input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 16)           0           graph_convolution_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "graph_convolution_4 (GraphConvo (None, 7)            119         dropout_4[0][0]                  \n",
            "                                                                 input_3[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 23,063\n",
            "Trainable params: 23,063\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkkOH-ITvSwb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Helper variables for main training loop\n",
        "wait = 0\n",
        "preds = None\n",
        "best_val_loss = 99999"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NS430VjjvWqp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "98159b0e-3da6-4c30-aabe-04592cf30c76"
      },
      "source": [
        "# Fit\n",
        "for epoch in range(1, NB_EPOCH+1):\n",
        "\n",
        "    # Log wall-clock time\n",
        "    t = time.time()\n",
        "\n",
        "    # Single training iteration (we mask nodes without labels for loss calculation)\n",
        "    model.fit(graph, y_train, sample_weight=train_mask,\n",
        "              batch_size=A.shape[0], epochs=1, shuffle=False, verbose=0)\n",
        "\n",
        "    # Predict on full dataset\n",
        "    preds = model.predict(graph, batch_size=A.shape[0])\n",
        "    \n",
        "    # Train / validation scores\n",
        "    train_val_loss, train_val_acc = utils.evaluate_preds(preds, [y_train, y_val],\n",
        "                                                   [idx_train, idx_val])\n",
        "    print(\"Epoch: {:04d}\".format(epoch),\n",
        "          \"train_loss= {:.4f}\".format(train_val_loss[0]),\n",
        "          \"train_acc= {:.4f}\".format(train_val_acc[0]),\n",
        "          \"val_loss= {:.4f}\".format(train_val_loss[1]),\n",
        "          \"val_acc= {:.4f}\".format(train_val_acc[1]),\n",
        "          \"time= {:.4f}\".format(time.time() - t))\n",
        "\n",
        "    # Early stopping\n",
        "    if train_val_loss[1] < best_val_loss:\n",
        "        best_val_loss = train_val_loss[1]\n",
        "        wait = 0\n",
        "    else:\n",
        "        if wait >= PATIENCE:\n",
        "            print('Epoch {}: early stopping'.format(epoch))\n",
        "            break\n",
        "        wait += 1\n",
        "\n",
        "# Testing\n",
        "test_loss, test_acc = utils.evaluate_preds(preds, [y_test], [idx_test])\n",
        "print(\"Test set results:\",\n",
        "      \"loss= {:.4f}\".format(test_loss[0]),\n",
        "      \"accuracy= {:.4f}\".format(test_acc[0]))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0001 train_loss= 1.9320 train_acc= 0.3000 val_loss= 1.9381 val_acc= 0.2024 time= 0.6628\n",
            "Epoch: 0002 train_loss= 1.9102 train_acc= 0.3875 val_loss= 1.9255 val_acc= 0.1976 time= 0.1261\n",
            "Epoch: 0003 train_loss= 1.8829 train_acc= 0.4250 val_loss= 1.9107 val_acc= 0.2214 time= 0.1332\n",
            "Epoch: 0004 train_loss= 1.8539 train_acc= 0.4500 val_loss= 1.8956 val_acc= 0.2357 time= 0.1250\n",
            "Epoch: 0005 train_loss= 1.8225 train_acc= 0.5000 val_loss= 1.8789 val_acc= 0.2738 time= 0.1271\n",
            "Epoch: 0006 train_loss= 1.7883 train_acc= 0.6250 val_loss= 1.8608 val_acc= 0.3429 time= 0.1287\n",
            "Epoch: 0007 train_loss= 1.7518 train_acc= 0.6875 val_loss= 1.8412 val_acc= 0.4214 time= 0.1304\n",
            "Epoch: 0008 train_loss= 1.7129 train_acc= 0.7500 val_loss= 1.8195 val_acc= 0.4810 time= 0.1364\n",
            "Epoch: 0009 train_loss= 1.6721 train_acc= 0.7875 val_loss= 1.7970 val_acc= 0.5310 time= 0.1269\n",
            "Epoch: 0010 train_loss= 1.6291 train_acc= 0.8250 val_loss= 1.7741 val_acc= 0.5857 time= 0.1281\n",
            "Epoch: 0011 train_loss= 1.5843 train_acc= 0.8375 val_loss= 1.7494 val_acc= 0.6143 time= 0.1319\n",
            "Epoch: 0012 train_loss= 1.5388 train_acc= 0.8500 val_loss= 1.7241 val_acc= 0.6548 time= 0.1263\n",
            "Epoch: 0013 train_loss= 1.4930 train_acc= 0.8500 val_loss= 1.6988 val_acc= 0.6714 time= 0.1297\n",
            "Epoch: 0014 train_loss= 1.4464 train_acc= 0.8500 val_loss= 1.6731 val_acc= 0.6857 time= 0.1276\n",
            "Epoch: 0015 train_loss= 1.3995 train_acc= 0.8500 val_loss= 1.6471 val_acc= 0.6929 time= 0.1261\n",
            "Epoch: 0016 train_loss= 1.3525 train_acc= 0.8500 val_loss= 1.6210 val_acc= 0.6952 time= 0.1256\n",
            "Epoch: 0017 train_loss= 1.3048 train_acc= 0.8500 val_loss= 1.5947 val_acc= 0.6976 time= 0.1303\n",
            "Epoch: 0018 train_loss= 1.2560 train_acc= 0.8500 val_loss= 1.5679 val_acc= 0.7024 time= 0.1260\n",
            "Epoch: 0019 train_loss= 1.2072 train_acc= 0.8625 val_loss= 1.5411 val_acc= 0.7048 time= 0.1278\n",
            "Epoch: 0020 train_loss= 1.1591 train_acc= 0.8625 val_loss= 1.5145 val_acc= 0.7071 time= 0.1299\n",
            "Epoch: 0021 train_loss= 1.1116 train_acc= 0.8625 val_loss= 1.4881 val_acc= 0.7143 time= 0.1256\n",
            "Epoch: 0022 train_loss= 1.0647 train_acc= 0.9000 val_loss= 1.4619 val_acc= 0.7143 time= 0.1197\n",
            "Epoch: 0023 train_loss= 1.0184 train_acc= 0.9125 val_loss= 1.4355 val_acc= 0.7190 time= 0.1332\n",
            "Epoch: 0024 train_loss= 0.9724 train_acc= 0.9125 val_loss= 1.4086 val_acc= 0.7262 time= 0.1281\n",
            "Epoch: 0025 train_loss= 0.9273 train_acc= 0.9375 val_loss= 1.3814 val_acc= 0.7190 time= 0.1279\n",
            "Epoch: 0026 train_loss= 0.8838 train_acc= 0.9375 val_loss= 1.3550 val_acc= 0.7238 time= 0.1299\n",
            "Epoch: 0027 train_loss= 0.8412 train_acc= 0.9500 val_loss= 1.3288 val_acc= 0.7405 time= 0.1252\n",
            "Epoch: 0028 train_loss= 0.8001 train_acc= 0.9500 val_loss= 1.3036 val_acc= 0.7452 time= 0.1315\n",
            "Epoch: 0029 train_loss= 0.7608 train_acc= 0.9500 val_loss= 1.2794 val_acc= 0.7571 time= 0.1255\n",
            "Epoch: 0030 train_loss= 0.7230 train_acc= 0.9500 val_loss= 1.2564 val_acc= 0.7595 time= 0.1320\n",
            "Epoch: 0031 train_loss= 0.6864 train_acc= 0.9500 val_loss= 1.2344 val_acc= 0.7595 time= 0.1265\n",
            "Epoch: 0032 train_loss= 0.6516 train_acc= 0.9500 val_loss= 1.2138 val_acc= 0.7643 time= 0.1209\n",
            "Epoch: 0033 train_loss= 0.6183 train_acc= 0.9500 val_loss= 1.1935 val_acc= 0.7667 time= 0.1266\n",
            "Epoch: 0034 train_loss= 0.5870 train_acc= 0.9500 val_loss= 1.1742 val_acc= 0.7667 time= 0.1265\n",
            "Epoch: 0035 train_loss= 0.5570 train_acc= 0.9500 val_loss= 1.1549 val_acc= 0.7667 time= 0.1325\n",
            "Epoch: 0036 train_loss= 0.5288 train_acc= 0.9500 val_loss= 1.1366 val_acc= 0.7667 time= 0.1257\n",
            "Epoch: 0037 train_loss= 0.5023 train_acc= 0.9500 val_loss= 1.1187 val_acc= 0.7738 time= 0.1312\n",
            "Epoch: 0038 train_loss= 0.4776 train_acc= 0.9500 val_loss= 1.1015 val_acc= 0.7738 time= 0.1294\n",
            "Epoch: 0039 train_loss= 0.4544 train_acc= 0.9500 val_loss= 1.0847 val_acc= 0.7762 time= 0.1280\n",
            "Epoch: 0040 train_loss= 0.4329 train_acc= 0.9500 val_loss= 1.0684 val_acc= 0.7762 time= 0.1267\n",
            "Epoch: 0041 train_loss= 0.4130 train_acc= 0.9500 val_loss= 1.0532 val_acc= 0.7738 time= 0.1277\n",
            "Epoch: 0042 train_loss= 0.3945 train_acc= 0.9500 val_loss= 1.0386 val_acc= 0.7738 time= 0.1279\n",
            "Epoch: 0043 train_loss= 0.3772 train_acc= 0.9625 val_loss= 1.0248 val_acc= 0.7714 time= 0.1252\n",
            "Epoch: 0044 train_loss= 0.3611 train_acc= 0.9625 val_loss= 1.0121 val_acc= 0.7714 time= 0.1303\n",
            "Epoch: 0045 train_loss= 0.3462 train_acc= 0.9625 val_loss= 1.0008 val_acc= 0.7690 time= 0.1296\n",
            "Epoch: 0046 train_loss= 0.3324 train_acc= 0.9750 val_loss= 0.9912 val_acc= 0.7690 time= 0.1241\n",
            "Epoch: 0047 train_loss= 0.3196 train_acc= 0.9750 val_loss= 0.9832 val_acc= 0.7667 time= 0.1322\n",
            "Epoch: 0048 train_loss= 0.3073 train_acc= 0.9750 val_loss= 0.9739 val_acc= 0.7714 time= 0.1248\n",
            "Epoch: 0049 train_loss= 0.2955 train_acc= 0.9750 val_loss= 0.9644 val_acc= 0.7714 time= 0.1332\n",
            "Epoch: 0050 train_loss= 0.2838 train_acc= 0.9750 val_loss= 0.9545 val_acc= 0.7714 time= 0.1221\n",
            "Epoch: 0051 train_loss= 0.2728 train_acc= 0.9750 val_loss= 0.9450 val_acc= 0.7738 time= 0.1280\n",
            "Epoch: 0052 train_loss= 0.2625 train_acc= 0.9750 val_loss= 0.9351 val_acc= 0.7738 time= 0.1269\n",
            "Epoch: 0053 train_loss= 0.2532 train_acc= 0.9750 val_loss= 0.9263 val_acc= 0.7738 time= 0.1258\n",
            "Epoch: 0054 train_loss= 0.2449 train_acc= 0.9750 val_loss= 0.9184 val_acc= 0.7738 time= 0.1263\n",
            "Epoch: 0055 train_loss= 0.2374 train_acc= 0.9750 val_loss= 0.9118 val_acc= 0.7738 time= 0.1243\n",
            "Epoch: 0056 train_loss= 0.2299 train_acc= 0.9750 val_loss= 0.9078 val_acc= 0.7738 time= 0.1206\n",
            "Epoch: 0057 train_loss= 0.2227 train_acc= 0.9750 val_loss= 0.9047 val_acc= 0.7738 time= 0.1318\n",
            "Epoch: 0058 train_loss= 0.2162 train_acc= 0.9750 val_loss= 0.9016 val_acc= 0.7762 time= 0.1245\n",
            "Epoch: 0059 train_loss= 0.2101 train_acc= 0.9750 val_loss= 0.8986 val_acc= 0.7714 time= 0.1328\n",
            "Epoch: 0060 train_loss= 0.2044 train_acc= 0.9750 val_loss= 0.8965 val_acc= 0.7738 time= 0.1269\n",
            "Epoch: 0061 train_loss= 0.1989 train_acc= 0.9750 val_loss= 0.8931 val_acc= 0.7714 time= 0.1307\n",
            "Epoch: 0062 train_loss= 0.1939 train_acc= 0.9750 val_loss= 0.8901 val_acc= 0.7690 time= 0.1275\n",
            "Epoch: 0063 train_loss= 0.1888 train_acc= 0.9750 val_loss= 0.8862 val_acc= 0.7667 time= 0.1286\n",
            "Epoch: 0064 train_loss= 0.1843 train_acc= 0.9750 val_loss= 0.8829 val_acc= 0.7667 time= 0.1329\n",
            "Epoch: 0065 train_loss= 0.1802 train_acc= 0.9750 val_loss= 0.8800 val_acc= 0.7667 time= 0.1273\n",
            "Epoch: 0066 train_loss= 0.1765 train_acc= 0.9750 val_loss= 0.8770 val_acc= 0.7667 time= 0.1227\n",
            "Epoch: 0067 train_loss= 0.1730 train_acc= 0.9750 val_loss= 0.8730 val_acc= 0.7667 time= 0.1291\n",
            "Epoch: 0068 train_loss= 0.1697 train_acc= 0.9750 val_loss= 0.8682 val_acc= 0.7667 time= 0.1263\n",
            "Epoch: 0069 train_loss= 0.1664 train_acc= 0.9750 val_loss= 0.8633 val_acc= 0.7667 time= 0.1307\n",
            "Epoch: 0070 train_loss= 0.1632 train_acc= 0.9750 val_loss= 0.8585 val_acc= 0.7667 time= 0.1287\n",
            "Epoch: 0071 train_loss= 0.1603 train_acc= 0.9875 val_loss= 0.8545 val_acc= 0.7667 time= 0.1265\n",
            "Epoch: 0072 train_loss= 0.1574 train_acc= 0.9875 val_loss= 0.8512 val_acc= 0.7714 time= 0.1309\n",
            "Epoch: 0073 train_loss= 0.1548 train_acc= 0.9875 val_loss= 0.8474 val_acc= 0.7738 time= 0.1268\n",
            "Epoch: 0074 train_loss= 0.1522 train_acc= 0.9875 val_loss= 0.8448 val_acc= 0.7714 time= 0.1309\n",
            "Epoch: 0075 train_loss= 0.1497 train_acc= 0.9875 val_loss= 0.8428 val_acc= 0.7690 time= 0.1314\n",
            "Epoch: 0076 train_loss= 0.1474 train_acc= 0.9875 val_loss= 0.8414 val_acc= 0.7667 time= 0.1460\n",
            "Epoch: 0077 train_loss= 0.1452 train_acc= 0.9875 val_loss= 0.8404 val_acc= 0.7690 time= 0.1358\n",
            "Epoch: 0078 train_loss= 0.1431 train_acc= 1.0000 val_loss= 0.8394 val_acc= 0.7714 time= 0.1227\n",
            "Epoch: 0079 train_loss= 0.1410 train_acc= 1.0000 val_loss= 0.8383 val_acc= 0.7714 time= 0.1324\n",
            "Epoch: 0080 train_loss= 0.1387 train_acc= 1.0000 val_loss= 0.8386 val_acc= 0.7738 time= 0.1217\n",
            "Epoch: 0081 train_loss= 0.1365 train_acc= 1.0000 val_loss= 0.8390 val_acc= 0.7810 time= 0.1286\n",
            "Epoch: 0082 train_loss= 0.1345 train_acc= 1.0000 val_loss= 0.8404 val_acc= 0.7786 time= 0.1242\n",
            "Epoch: 0083 train_loss= 0.1327 train_acc= 1.0000 val_loss= 0.8416 val_acc= 0.7810 time= 0.1280\n",
            "Epoch: 0084 train_loss= 0.1308 train_acc= 1.0000 val_loss= 0.8409 val_acc= 0.7833 time= 0.1270\n",
            "Epoch: 0085 train_loss= 0.1286 train_acc= 1.0000 val_loss= 0.8397 val_acc= 0.7833 time= 0.1267\n",
            "Epoch: 0086 train_loss= 0.1264 train_acc= 1.0000 val_loss= 0.8383 val_acc= 0.7810 time= 0.1297\n",
            "Epoch: 0087 train_loss= 0.1245 train_acc= 1.0000 val_loss= 0.8367 val_acc= 0.7762 time= 0.1281\n",
            "Epoch: 0088 train_loss= 0.1226 train_acc= 1.0000 val_loss= 0.8351 val_acc= 0.7714 time= 0.1272\n",
            "Epoch: 0089 train_loss= 0.1209 train_acc= 1.0000 val_loss= 0.8327 val_acc= 0.7690 time= 0.1330\n",
            "Epoch: 0090 train_loss= 0.1192 train_acc= 1.0000 val_loss= 0.8290 val_acc= 0.7714 time= 0.1273\n",
            "Epoch: 0091 train_loss= 0.1175 train_acc= 1.0000 val_loss= 0.8267 val_acc= 0.7714 time= 0.1347\n",
            "Epoch: 0092 train_loss= 0.1161 train_acc= 1.0000 val_loss= 0.8281 val_acc= 0.7714 time= 0.1262\n",
            "Epoch: 0093 train_loss= 0.1149 train_acc= 1.0000 val_loss= 0.8292 val_acc= 0.7738 time= 0.1344\n",
            "Epoch: 0094 train_loss= 0.1136 train_acc= 1.0000 val_loss= 0.8301 val_acc= 0.7714 time= 0.1251\n",
            "Epoch: 0095 train_loss= 0.1125 train_acc= 1.0000 val_loss= 0.8332 val_acc= 0.7714 time= 0.1316\n",
            "Epoch: 0096 train_loss= 0.1111 train_acc= 1.0000 val_loss= 0.8317 val_acc= 0.7714 time= 0.1287\n",
            "Epoch: 0097 train_loss= 0.1098 train_acc= 1.0000 val_loss= 0.8305 val_acc= 0.7714 time= 0.1312\n",
            "Epoch: 0098 train_loss= 0.1085 train_acc= 1.0000 val_loss= 0.8286 val_acc= 0.7714 time= 0.1329\n",
            "Epoch: 0099 train_loss= 0.1072 train_acc= 1.0000 val_loss= 0.8260 val_acc= 0.7714 time= 0.1338\n",
            "Epoch: 0100 train_loss= 0.1052 train_acc= 1.0000 val_loss= 0.8177 val_acc= 0.7714 time= 0.1331\n",
            "Epoch: 0101 train_loss= 0.1033 train_acc= 1.0000 val_loss= 0.8096 val_acc= 0.7762 time= 0.1318\n",
            "Epoch: 0102 train_loss= 0.1019 train_acc= 1.0000 val_loss= 0.8023 val_acc= 0.7762 time= 0.1294\n",
            "Epoch: 0103 train_loss= 0.1007 train_acc= 1.0000 val_loss= 0.7971 val_acc= 0.7738 time= 0.1280\n",
            "Epoch: 0104 train_loss= 0.0998 train_acc= 1.0000 val_loss= 0.7936 val_acc= 0.7714 time= 0.1257\n",
            "Epoch: 0105 train_loss= 0.0992 train_acc= 1.0000 val_loss= 0.7901 val_acc= 0.7714 time= 0.1344\n",
            "Epoch: 0106 train_loss= 0.0986 train_acc= 1.0000 val_loss= 0.7876 val_acc= 0.7714 time= 0.1315\n",
            "Epoch: 0107 train_loss= 0.0975 train_acc= 1.0000 val_loss= 0.7869 val_acc= 0.7738 time= 0.1389\n",
            "Epoch: 0108 train_loss= 0.0962 train_acc= 1.0000 val_loss= 0.7863 val_acc= 0.7714 time= 0.1268\n",
            "Epoch: 0109 train_loss= 0.0951 train_acc= 1.0000 val_loss= 0.7851 val_acc= 0.7690 time= 0.1326\n",
            "Epoch: 0110 train_loss= 0.0939 train_acc= 1.0000 val_loss= 0.7850 val_acc= 0.7714 time= 0.1292\n",
            "Epoch: 0111 train_loss= 0.0928 train_acc= 1.0000 val_loss= 0.7858 val_acc= 0.7690 time= 0.1267\n",
            "Epoch: 0112 train_loss= 0.0918 train_acc= 1.0000 val_loss= 0.7880 val_acc= 0.7690 time= 0.1355\n",
            "Epoch: 0113 train_loss= 0.0911 train_acc= 1.0000 val_loss= 0.7917 val_acc= 0.7738 time= 0.1314\n",
            "Epoch: 0114 train_loss= 0.0905 train_acc= 1.0000 val_loss= 0.7956 val_acc= 0.7738 time= 0.1303\n",
            "Epoch: 0115 train_loss= 0.0898 train_acc= 1.0000 val_loss= 0.7960 val_acc= 0.7738 time= 0.1309\n",
            "Epoch: 0116 train_loss= 0.0890 train_acc= 1.0000 val_loss= 0.7949 val_acc= 0.7738 time= 0.1346\n",
            "Epoch: 0117 train_loss= 0.0881 train_acc= 1.0000 val_loss= 0.7927 val_acc= 0.7762 time= 0.1283\n",
            "Epoch: 0118 train_loss= 0.0871 train_acc= 1.0000 val_loss= 0.7910 val_acc= 0.7786 time= 0.1270\n",
            "Epoch: 0119 train_loss= 0.0859 train_acc= 1.0000 val_loss= 0.7871 val_acc= 0.7810 time= 0.1307\n",
            "Epoch: 0120 train_loss= 0.0846 train_acc= 1.0000 val_loss= 0.7837 val_acc= 0.7857 time= 0.1276\n",
            "Epoch: 0121 train_loss= 0.0836 train_acc= 1.0000 val_loss= 0.7786 val_acc= 0.7810 time= 0.1365\n",
            "Epoch: 0122 train_loss= 0.0826 train_acc= 1.0000 val_loss= 0.7747 val_acc= 0.7810 time= 0.1288\n",
            "Epoch: 0123 train_loss= 0.0818 train_acc= 1.0000 val_loss= 0.7722 val_acc= 0.7810 time= 0.1338\n",
            "Epoch: 0124 train_loss= 0.0811 train_acc= 1.0000 val_loss= 0.7712 val_acc= 0.7786 time= 0.1295\n",
            "Epoch: 0125 train_loss= 0.0803 train_acc= 1.0000 val_loss= 0.7713 val_acc= 0.7786 time= 0.1335\n",
            "Epoch: 0126 train_loss= 0.0794 train_acc= 1.0000 val_loss= 0.7718 val_acc= 0.7786 time= 0.1357\n",
            "Epoch: 0127 train_loss= 0.0785 train_acc= 1.0000 val_loss= 0.7736 val_acc= 0.7762 time= 0.1352\n",
            "Epoch: 0128 train_loss= 0.0775 train_acc= 1.0000 val_loss= 0.7749 val_acc= 0.7786 time= 0.1299\n",
            "Epoch: 0129 train_loss= 0.0767 train_acc= 1.0000 val_loss= 0.7764 val_acc= 0.7786 time= 0.1312\n",
            "Epoch: 0130 train_loss= 0.0759 train_acc= 1.0000 val_loss= 0.7771 val_acc= 0.7810 time= 0.1309\n",
            "Epoch: 0131 train_loss= 0.0755 train_acc= 1.0000 val_loss= 0.7808 val_acc= 0.7857 time= 0.1326\n",
            "Epoch: 0132 train_loss= 0.0755 train_acc= 1.0000 val_loss= 0.7845 val_acc= 0.7905 time= 0.1274\n",
            "Epoch: 0133 train_loss= 0.0754 train_acc= 1.0000 val_loss= 0.7850 val_acc= 0.7929 time= 0.1327\n",
            "Epoch: 0134 train_loss= 0.0753 train_acc= 1.0000 val_loss= 0.7849 val_acc= 0.7857 time= 0.1302\n",
            "Epoch: 0135 train_loss= 0.0749 train_acc= 1.0000 val_loss= 0.7817 val_acc= 0.7857 time= 0.1321\n",
            "Epoch 135: early stopping\n",
            "Test set results: loss= 0.8003 accuracy= 0.7600\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivDThlJqvZuB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}